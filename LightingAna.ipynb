{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Overview: \n",
    "This dataset contains cloud-to-ground lightning strike information collected by Vaisala's National Lightning Detection Network and aggregated into 1.1 degree tiles by the experts at the National Centers for Environmental Information (NCEI) as part of their Severe Weather Data Inventory. This data provides historical cloud-to-ground data aggregated into tiles that around roughly 11 KMs for redistribution. This provides users with the number of lightning strikes each day, as well as the center point for each tile. The sample queries below will help you get started using BigQuery's GIS capabilities to analyze the data. \n",
    "\n",
    "Update frequency: Daily\n",
    "\n",
    "Dataset source: NOAA\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`date`:\tDATE\tNULLABLE\n",
    "\n",
    "`number_of_strikes`:\tINTEGER\tNULLABLE\n",
    "\n",
    "`center_point_geom`:\tGEOGRAPHY\tNULLABLE\t\n",
    "*Center point of 0.10-degree tiles (roughly 1.1km) that aggregate strikes within the given tile.*\n",
    "\n",
    "**Point ( Lat, Long, SRID )**\n",
    "\n",
    "`Lat`\n",
    "\n",
    "Is a float expression representing the y-coordinate of the Point being generated.\n",
    "\n",
    "`Long`\n",
    "\n",
    "Is a float expression representing the x-coordinate of the Point being generated. For more information on valid latitude and longitude values, see <a href=\"url\" target=\"_blank\">Point</a>.\n",
    "\n",
    "`SRID`\n",
    "\n",
    "Is an int expression representing the Spatial Reference Identifier of the geography instance you wish to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schemaFields = [StructField(\"date\", DateType(), True),\n",
    "                StructField(\"number_of_strikes\",  IntegerType(), True),\n",
    "                StructField(\"center_point_geom\", StringType(), True)]\n",
    "Schema = StructType(schemaFields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark1 = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"LightingAna\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_lighting_data= spark1\\\n",
    ".read\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".schema(Schema)\\\n",
    ".csv(r\"C:\\Users\\USER\\Desktop\\GCP\\mypro_tiw\\lightingData\\year1987.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- number_of_strikes: integer (nullable = true)\n",
      " |-- center_point_geom: string (nullable = true)\n",
      "\n",
      "+----------+-----------------+-----------------+\n",
      "|      date|number_of_strikes|center_point_geom|\n",
      "+----------+-----------------+-----------------+\n",
      "|1987-01-01|               21|  POINT(-80.7 26)|\n",
      "|1987-01-04|               23|POINT(-83.2 28.3)|\n",
      "|1987-01-05|               40|POINT(-78.9 31.3)|\n",
      "|1987-01-05|               20|POINT(-79.2 31.4)|\n",
      "|1987-01-05|               21|POINT(-80.1 31.4)|\n",
      "|1987-01-05|               18|POINT(-79.4 31.5)|\n",
      "|1987-01-05|               20|POINT(-78.8 31.3)|\n",
      "|1987-01-05|               35|  POINT(-80 31.4)|\n",
      "|1987-01-17|               20|POINT(-77.4 31.9)|\n",
      "|1987-01-17|               17|  POINT(-77 31.9)|\n",
      "|1987-01-17|               20|POINT(-77.2 31.8)|\n",
      "|1987-01-17|               24|POINT(-77.4 31.8)|\n",
      "|1987-01-25|               17|  POINT(-91.2 32)|\n",
      "|1987-01-25|               20|POINT(-91.6 32.5)|\n",
      "|1987-01-25|               17|POINT(-91.7 32.1)|\n",
      "|1987-01-31|               17|POINT(-76.8 32.4)|\n",
      "|1987-01-31|               20|POINT(-76.1 32.5)|\n",
      "|1987-01-31|               17|POINT(-78.2 31.9)|\n",
      "|1987-01-31|               17|POINT(-76.7 32.5)|\n",
      "|1987-02-06|               32|POINT(-80.4 28.9)|\n",
      "+----------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_lighting_data.printSchema()\n",
    "raw_lighting_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_to_LAT(point):\n",
    "    raw = point.split()\n",
    "    LAT = float(raw[0][6:])\n",
    "    return LAT\n",
    "def geo_to_LONG(point):\n",
    "    raw = point.split()\n",
    "    LONG = float(raw[1][:-1])\n",
    "    return LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+-----+----+\n",
      "|      date|number_of_strikes|center_point_geom|  LAT|LONG|\n",
      "+----------+-----------------+-----------------+-----+----+\n",
      "|1987-01-01|               21|  POINT(-80.7 26)|-80.7|26.0|\n",
      "|1987-01-04|               23|POINT(-83.2 28.3)|-83.2|28.3|\n",
      "|1987-01-05|               40|POINT(-78.9 31.3)|-78.9|31.3|\n",
      "|1987-01-05|               20|POINT(-79.2 31.4)|-79.2|31.4|\n",
      "|1987-01-05|               21|POINT(-80.1 31.4)|-80.1|31.4|\n",
      "|1987-01-05|               18|POINT(-79.4 31.5)|-79.4|31.5|\n",
      "|1987-01-05|               20|POINT(-78.8 31.3)|-78.8|31.3|\n",
      "|1987-01-05|               35|  POINT(-80 31.4)|-80.0|31.4|\n",
      "|1987-01-17|               20|POINT(-77.4 31.9)|-77.4|31.9|\n",
      "|1987-01-17|               17|  POINT(-77 31.9)|-77.0|31.9|\n",
      "|1987-01-17|               20|POINT(-77.2 31.8)|-77.2|31.8|\n",
      "|1987-01-17|               24|POINT(-77.4 31.8)|-77.4|31.8|\n",
      "|1987-01-25|               17|  POINT(-91.2 32)|-91.2|32.0|\n",
      "|1987-01-25|               20|POINT(-91.6 32.5)|-91.6|32.5|\n",
      "|1987-01-25|               17|POINT(-91.7 32.1)|-91.7|32.1|\n",
      "|1987-01-31|               17|POINT(-76.8 32.4)|-76.8|32.4|\n",
      "|1987-01-31|               20|POINT(-76.1 32.5)|-76.1|32.5|\n",
      "|1987-01-31|               17|POINT(-78.2 31.9)|-78.2|31.9|\n",
      "|1987-01-31|               17|POINT(-76.7 32.5)|-76.7|32.5|\n",
      "|1987-02-06|               32|POINT(-80.4 28.9)|-80.4|28.9|\n",
      "+----------+-----------------+-----------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "point_to_LAT = udf(lambda point: geo_to_LAT(point), FloatType())\n",
    "point_to_LONG = udf(lambda point: geo_to_LONG(point), FloatType())\n",
    "data_update_geo = raw_lighting_data.withColumn(\"LAT\", point_to_LAT(raw_lighting_data.center_point_geom))\\\n",
    "                            .withColumn(\"LONG\", point_to_LONG(raw_lighting_data.center_point_geom))\n",
    "data_update_geo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find**  min, max of LAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df, columns):\n",
    "    if isinstance(columns, dict):\n",
    "        for old_name, new_name in columns.items():\n",
    "            df = df.withColumnRenamed(old_name, new_name)\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"'columns' should be a dict, like {'old_name_1':'new_name_1', 'old_name_2':'new_name_2'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|max_Latitude|min_Latitude|\n",
      "+------------+------------+\n",
      "|       -60.0|      -130.0|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max,min\n",
    "renameDict = {'max(LAT)':'max_Latitude','min(LAT)':'min_Latitude'}\n",
    "MinMaxLAT = data_update_geo.select(max(\"LAT\"),min(\"LAT\"))\n",
    "MinMaxLAT = rename_columns(MinMaxLAT, renameDict)\n",
    "MinMaxLAT.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min max2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}